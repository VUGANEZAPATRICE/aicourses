# -*- coding: utf-8 -*-
"""Patrice Vuganeza Hackathon File.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cK9w0DzkPKVQGacUDE8rR8IFLsls81Ty
"""

from google.colab import drive
drive.mount("/content/Drive")

import pandas as pd
from bs4 import BeautifulSoup
import requests

"""# Scraping only jobs from jobinrwanda"""

content_jobs = requests.get("https://www.jobinrwanda.com/jobs/all")
soup =  BeautifulSoup(content_jobs.content, "html.parser")

# function to loop through all jobs from main page
def parsing_job_in_rwanda(soup):
  main_dict = {}
  adverts_jobs = soup.find_all("div", class_="card-body p-2")
  job_title_list =[]
  company_name_list= []
  job_link_list=[]
  #company_profile = []
  company_link_list=[]
  #job_description = []
  category_list =[]
  experience_list = []
  published_date_list =[]
  deadline_list =[]

  for index, advert_job in enumerate(adverts_jobs):
    title = adverts_jobs[index].find("span", class_="field field--name-title field--type-string field--label-hidden").text
    title_link = "https://www.jobinrwanda.com"+adverts_jobs[index].find('a')['href']
    company_link = "https://www.jobinrwanda.com"+ adverts_jobs[index].find("p",class_="card-text").find("a")["href"]
    company_name = adverts_jobs[index].find("p",class_="card-text").find("a").text
    published_date = adverts_jobs[index].find("p", class_="card-text").text.split("\n")[2].strip().split("|")[0]
    deadline = adverts_jobs[index].find('time').text
    experience = adverts_jobs[index].find("p",class_="card-text").text.split("\n")[5].strip()
    #category = adverts_jobs[index].find("p", class_="card-text").find("span", class_="badge badge-primary").text
    category = adverts_jobs[index].find("p",class_="card-text").find("span",class_="badge badge-primary").getText()
               #job_posts[0].find("p",class_="card-text").find("span",class_="badge badge-primary").text
    job_title_list.append(title)
    company_name_list.append(company_name)
    job_link_list.append(title_link)
    company_link_list.append(company_link)
    category_list.append(category)
    experience_list.append(experience)
    published_date_list.append(published_date)
    deadline_list.append(deadline)

  main_dict["job_title"] = job_title_list
  main_dict["company_name"] = company_name_list
  main_dict["job_link"] = job_link_list
  main_dict["company_link"] = company_link_list
  main_dict["job_category"] = category_list
  main_dict["job_experience"] = experience_list
  main_dict["published_date"] = published_date_list
  main_dict["job_deadline"] = deadline_list

  return main_dict

main_dict_here =parsing_job_in_rwanda(soup)
main_dict_here

# looping through job description

#employer_page[0].text
company_profile = []
company_description = []
job_summary= []

for link in main_dict_here["job_link"]:
  company_page = requests.get(link).content
  #job_description_raw_content = requests.get('https://www.jobinrwanda.com/job/global-remote-sensing-manager').content

  soup_company_page = BeautifulSoup(company_page,'html.parser')
  employer_page = soup_company_page.find_all('div', class_='employer-description') #Employer Description
  describe = soup_company_page.find_all('div', class_="clearfix text-formatted field field--name-field-job-full-description field--type-text-long field--label-hidden field__item") #Employer Description
  company_prof = ''
  company_desc = ''
   #Construct an empty string to hold the paragraph
  for index,emp_page in enumerate(employer_page):
    for textt in employer_page[0].text.split('\n'):
      company_prof += textt +'\n'
    description1 = ''
    for paragraph in describe:
      description1 += paragraph.getText() #Adding each paragraph we find in the job description.

    for textt in description1.split('\n'):
      company_desc += textt +'\n'

  #summary_content =requests.get("https://www.jobinrwanda.com/job/customer-service-officer-1").content
  #s_soup = BeautifulSoup(summary_content,'html.parser')
  soup_company_page=BeautifulSoup(company_page,'html.parser')
  #s_soup1 = soup_company_page.find("div",class_="card-header bg-primary").getText().strip()
  #print(s_soup1)
  s_soup2 = soup_company_page.find_all("div",class_="card border-primary sidebar-block")[0].find_all('ul', class_="list-group list-group-flush")[0].find_all('li')[:7]
  summary1=''
  for ss in s_soup2:
    summary1 +=ss.getText().strip()

  
  company_description.append(company_desc)
  company_profile.append(company_prof)
  job_summary.append(summary1)

# Adding Job description to dictionary

print(len(company_profile))
print(company_profile)
main_dict_here["Company_Profile"] = company_profile
main_dict_here["company_description"] = company_description
main_dict_here["job_summary"] = job_summary

print(main_dict_here['job_summary'])

# Creating dataframe for jobs
df = pd.DataFrame(main_dict_here)
df

"""# scraping Consultancy"""

content_jobs = requests.get("https://www.jobinrwanda.com/jobs/consultancy")
soup =  BeautifulSoup(content_jobs.content, "html.parser")

def parsing_job_in_rwanda1(soup):
  main_dict = {}
  adverts_jobs = soup.find_all("div", class_="card-body p-2")
  job_title_list =[]
  company_name_list= []
  job_link_list=[]
  #company_profile = []
  company_link_list=[]
  #job_description = []
  category_list =[]
  experience_list = []
  published_date_list =[]
  deadline_list =[]

  for index, advert_job in enumerate(adverts_jobs):
    title = adverts_jobs[index].find("span", class_="field field--name-title field--type-string field--label-hidden").text
    title_link = "https://www.jobinrwanda.com"+adverts_jobs[index].find('a')['href']
    company_link = "https://www.jobinrwanda.com"+ adverts_jobs[index].find("p",class_="card-text").find("a")["href"]
    company_name = adverts_jobs[index].find("p",class_="card-text").find("a").text
    published_date = adverts_jobs[index].find("p", class_="card-text").text.split("\n")[2].strip()
    deadline = adverts_jobs[index].find('time').text
    experience = adverts_jobs[index].find("p",class_="card-text").text.split("\n")[5].strip()
    #category = adverts_jobs[index].find("p", class_="card-text").find("span", class_="badge badge-primary").text
    category = adverts_jobs[index].find("p",class_="card-text").find("span",class_="badge badge-success").getText()
               #job_posts[0].find("p",class_="card-text").find("span",class_="badge badge-primary").text
    job_title_list.append(title)
    company_name_list.append(company_name)
    job_link_list.append(title_link)
    company_link_list.append(company_link)
    category_list.append(category)
    experience_list.append(experience)
    published_date_list.append(published_date)
    deadline_list.append(deadline)

  main_dict["job_title"] = job_title_list
  main_dict["company_name"] = company_name_list
  main_dict["job_link"] = job_link_list
  main_dict["company_link"] = company_link_list
  main_dict["job_category"] = category_list
  main_dict["job_experience"] = experience_list
  main_dict["published_date"] = published_date_list
  main_dict["job_deadline"] = deadline_list

  return main_dict

main_dict_here1 =parsing_job_in_rwanda1(soup)
main_dict_here1

# looping through consultancy description

company_profile1 = []
company_description1 = []
job_summary11= []

for link in main_dict_here1["job_link"]:
  company_page = requests.get(link).content
  #job_description_raw_content = requests.get('https://www.jobinrwanda.com/job/global-remote-sensing-manager').content

  soup_company_page = BeautifulSoup(company_page,'html.parser')
  employer_page = soup_company_page.find_all('div', class_='employer-description') #Employer Description
  describe = soup_company_page.find_all('div', class_="clearfix text-formatted field field--name-field-job-full-description field--type-text-long field--label-hidden field__item") #Employer Description
  company_prof = ''
  company_desc = ''
   #Construct an empty string to hold the paragraph
  for index,emp_page in enumerate(employer_page):
    for textt in employer_page[0].text.split('\n'):
      company_prof += textt +'\n'
    description1 = ''
    for paragraph in describe:
      description1 += paragraph.getText() #Adding each paragraph we find in the job description.

  for textt in description1.split('\n'):
    company_desc += textt +'\n'
  soup_company_page=BeautifulSoup(company_page,'html.parser')
  #s_soup1 = soup_company_page.find("div",class_="card-header bg-primary").getText().strip()
  #print(s_soup1)
  s_soup2 = soup_company_page.find_all("div",class_="card border-primary sidebar-block")[0].find_all('ul', class_="list-group list-group-flush")[0].find_all('li')[:7]
  summary1=''
  for ss in s_soup2:
    summary1 +=ss.getText().strip()

  
  company_description1.append(company_desc)
  company_profile1.append(company_prof)
  job_summary11.append(summary1)

main_dict_here1["Company_Profile"] = company_profile1
main_dict_here1["company_description"] = company_description1
main_dict_here1["job_summary"] = job_summary11

print(len(main_dict_here1))

df1 = pd.DataFrame(main_dict_here1)
df1

df1["job_summary"]

df1.info()

df.info()

"""# Concatenating two data frames"""

df_result = pd.concat([df, df1], ignore_index=True, sort=False)
df_result

df_result.info()

"""# IT related jobs from the above dataset"""

import re
tags = "#ICT#informationtechnology #technology #it #cybersecurity #tech #computerscience #programming  #coding #innovation #software #python #information #computer #informationsecurity #security #technologynews #java #networking #hacking #programmer #linux #technologyrocks #coder #technologythesedays #cloudcomputing  #engineering #itservices #newtechnology"
sss = tags.split('#')[1:]
sss_strip =[]
for tag in sss:
  sss_strip.append(tag.strip())
sss_strip

# Creating an empty dataframe to store IT related Jobs

df_jobs = pd.DataFrame()
df_jobs

for index1, row in enumerate(df_result['job_summary']):
  for it_job in sss_strip:
    pattern = re.compile(f"\s?{it_job}\s?")
    #pattern = re.compile(f"\s?{it}\s?")
    matches = pattern.findall(str(df_result["job_summary"][index1].split("\n")[2].split("Other")[0].strip().lower()))
    if matches != []:
      df3=df_result.iloc[index1:index1+1,:]
      df_jobs=pd.concat([df_jobs,df3])

df_jobs

df_jobs.info()

"""# Translating"""

!pip install -U easynmt

from easynmt import EasyNMT
#model = EasyNMT('opus-mt')
#model = EasyNMT('opus-mt', max_loaded_models=10)
model = EasyNMT('mbart50_en2m')


this_arr=[]

df_translated = pd.DataFrame()
for transl_col in ['job_title', 'company_name','job_experience','published_date','job_deadline','Company_Profile','company_description', 'job_summary']:
  

  for index2, row2 in enumerate(df_result[transl_col]):
  
    Translations = model.translate(row2,target_lang='fr') 
    df_result[transl_col][index2]=Translations
    this_arr.append(Translations)
    df4=df_result.iloc[index2:index2+1,:]
    df_translated=pd.concat([df_translated,df4])
  
df_translated

df_translated['job_summary']

df_translated['company_description']

"""# UMUCYO Tenders"""

umucyo_web_page =  requests.get('https://www.umucyo.gov.rw/eb/bav/selectListAdvertisingListForGU.do').content

umucyo_soup = BeautifulSoup(umucyo_web_page,'html.parser')
table = umucyo_soup.find_all("table", class_="article_table mb10")
th = table[0].find_all("thead")[0].find("tr").find_all("th")
th

trs = table[0].find("tbody").find_all("tr")
td = trs[0].find_all("td")
print(len(td))
print(len(th))
print(td)

for i in th:
  print(i.text)

columnsa = []

for i in th:
  regla = re.findall(r"[a-zA-Z0-9 ]+", i.text)
  for cols in regla:

    columnsa.append(cols)

print(columnsa)

df_umucyo = pd.DataFrame(columns=columnsa)
df_umucyo

later_rows= []
trs = table[0].find_all("tbody")[0].find_all("tr")

for td in trs:
  td_content = td.find_all("td")
  single_row=[]
  for td_t in td_content[1:]:
    f_data = td_t.text.strip()
    single_row.append(f_data)

  later_rows.append(single_row)
 

later_rows

df_umucyo1=pd.DataFrame(later_rows,columns=columnsa)
df_umucyo1

"""# Function to get job adverts in more than 50+"""

!pip install easynmt

from easynmt import EasyNMT
#model = EasyNMT('opus-mt')
#model = EasyNMT('opus-mt', max_loaded_models=10)
model = EasyNMT('mbart50_en2m')


def adv_translate(language_t):

  this_arr=[]

  df_translated1 = pd.DataFrame()
  for transl_col in ['job_title', 'company_name','job_experience','published_date','job_deadline','Company_Profile','company_description', 'job_summary']:
    

    for index2, row2 in enumerate(df_result[transl_col]):
    
      Translations = model.translate(row2,target_lang=f'{language_t}') 
      df_result[transl_col][index2]=Translations
      this_arr.append(Translations)
      df4=df_result.iloc[index2:index2+1,:]
      df_translated=pd.concat([df_translated,df4])
  return df_translated1





"""# Fast API"""

!pip install fastapi pyngrok unicorn nest-asyncio

!pip install uvicorn

!ngrok authtoken 2HJPnhvlBWISNubtEkRsF33F1JR_7srEdMeL114tfLGMQ7awh

!pip install easynmt

df_result[['job_title','job_summary']]



from fastapi import FastAPI, Request
from datetime import datetime
#importing easy nmt
from easynmt import EasyNMT
from fastapi.responses import HTMLResponse

#defining our model or startying the model instance

model = EasyNMT("opus-mt")

storage = FastAPI(title='My FastAPI')

#df_jobs.to_dict('r')#IT
#df_umucyo1.to_dict('r')
#df_result.to_dict('r')#all jobs
#trans = adv_translate().to_dict('r')# to translate for you

#  fast API way
@storage.get('/', response_class=HTMLResponse)
def index():
    return f"""<html>
  <h1>Here is the first page: please Pay attention to details</h1><br><br>
   <p> fill with the corresponding urls to see:
  <h3>IT Jobs</h3>: <p>/it-jobs</p><br><br>
  <h3>All jobs</h3>: <p>/jobinrwanda-job-data</p><br><br>
  <h3>Umucyo Tenders</h3>: <p>/umucyo-tenders</p><br><br>
  <h3>to trnslate in your own language</h3>: <p></p>translate<br><br>
  
  </html>
  """
#==========================================================================================
@storage.get('/translated-dataFrame')
def translated_data_frame(category: str = None,job_title:str = None):

  if category:

    return df_jobs[['job_title','job_summary','published_date','job_deadline']].to_dict('r') 
  elif job_title:
    return df_jobs[['job_title','published_date','job_deadline']].to_dict('r')

  else:
    return df_translated.to_dict('r')
#=========================================================================================
#=========================================================================================
@storage.get('/umucyo-tenders')
def show_it_job_data(category: str = None,job_title:str = None):
  if category:
    return df_umucyo1[['Tender Name','Tender No','Deadline of Submitting']].to_dict('r') 
  elif job_title:
    return df_umucyo1[['Tender Name','Deadline of Submitting']].to_dict('r')

  else:
    return df_umucyo1.to_dict('r')
#=========================================================================================

#=======================================================================================
@storage.get('/it-jobs')
def show_it_job_data(category: str = None,job_title:str = None):
  if category:
    return df_jobs[['job_title','job_summary','published_date','job_deadline']].to_dict('r') 
  elif job_title:
    return df_jobs[['job_title','published_date','job_deadline']].to_dict('r')

  else:
    return df_jobs.to_dict('r')
#=========================================================================================
@storage.get('/translation')
def translate(text: str="",language_t: str=""):
  response = adv_translate(language_t).to_dict('r')
  return response

#============================================================================================

@storage.get('/jobinrwanda-job-data')
def show_job_data(category: str = None,job_title:str = None):
  if category:
    return df_result[['job_title','job_summary','published_date','job_deadline']].to_dict('r') 
  elif job_title:
    return df_result[['job_title','published_date','job_deadline']].to_dict('r')

  else:
    return df_result.to_dict('r')

#============================================================================================

import nest_asyncio
from pyngrok import ngrok
import uvicorn

ngrok_tunnel = ngrok.connect(8000)
print("REST API started")
print("Your public API URL:", ngrok_tunnel.public_url)
print("You can for example open the following URL in your browser: {}?target_lang=en&text=Hallo%20Welt".format(ngrok_tunnel.public_url))

nest_asyncio.apply()
uvicorn.run(storage, port=8000)

columns=[ 'job_title','company_name','job_link','company_link','job_category','job_experience','published_date','job_deadline','Company_Profile','company_description','job_summary']



